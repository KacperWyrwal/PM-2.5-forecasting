{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b707efd0",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08721db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f672c",
   "metadata": {},
   "source": [
    "# Other requirements \n",
    "You will also need to download a __Microsoft Edge Webdriver__ (e.g. from __https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/__). This scraper works with Microsoft Edge by default; however, it can be easily adjusted to any common browser by using a different webdriver e.g. Chrome webdriver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Web Driver \n",
    "edge_driver_path = \"C:/Users/wyrwa/Speckled/msedgedriver.exe\"\n",
    "driver = webdriver.Edge(edge_driver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad0bca",
   "metadata": {},
   "source": [
    "# Select target URL\n",
    "This scraper definitely works with the URL given below (Indira Gandhi Airport); however, it will also work with most other location; thus, when looking for weather data in different locations, replace the __URL__ variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the URL you want to execute JavaScript commands on\n",
    "URL = 'https://rp5.ru/Weather_archive_in_New_Delhi,_Indira_Gandhi_(airport),_METAR'\n",
    "driver.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe06f6b",
   "metadata": {},
   "source": [
    "# Main scraper body\n",
    "The main, and the only essential, part of the scraper is the generator below. For each of the dates between (inclusive) the dates given, it fetches a dataframe of weather data. Note that the format of the arguments is __MM/DD/YYYY__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c84b0db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_gen(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    start_date: MM/DD/YYYY (inclusive)\n",
    "    end_date: MM/DD/YYYY (inclusive)\n",
    "    \"\"\"\n",
    "    for date in pd.date_range(start_date, end_date):\n",
    "        # Query data for the given date using a JS command \n",
    "        driver.execute_script(\n",
    "            f\"jQuery.datepick._selectDate('#calender_archive', '{date.strftime('%d.%m.%Y')}'); fMetarConfirm()\"\n",
    "        )\n",
    "        # Yield dataframe for given day\n",
    "        yield pd.read_html(str(BeautifulSoup(driver.page_source, 'html.parser').select_one('#archiveTable')))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8ef3c",
   "metadata": {},
   "source": [
    "# Data processing \n",
    "This processing below is specific to the Indira Gandhi Airport data; however, it will likely be helpful for other locations as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10e0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    This function will be executed on each dataframe yielded by scrape_gen. \n",
    "    \"\"\"\n",
    "    # The first row in the dataframe contains column names, we remove it.\n",
    "    return df.drop(df.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1dd8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names and date format for Indira Gandhi Airport data\n",
    "COLUMNS = [\n",
    "        'Timestamp', 'Temperature', 'Pressure (Station)', 'Pressure', 'Humidity', 'Wind Direction', \n",
    "        'Wind Speed', 'Gust', 'Phenomenon', 'Phenomenon (Other)', 'Clouds', 'Visibility', 'Dewpoint',\n",
    "]\n",
    "DATE_FORMAT = '%Y%B\\xa0%d,%A%H:%M'\n",
    "\n",
    "\n",
    "def postprocess(df, start_date=None, end_date=None, inplace=False):\n",
    "    \"\"\"\n",
    "    This function will be applied to the dataframe combined from all dataframes yielded from scrape_gen.\n",
    "    \"\"\"\n",
    "    if inplace is False: \n",
    "        df = df.copy() \n",
    "        \n",
    "    # Intially, there are two columns for the date and time of the measurement. We merge them together and \n",
    "    # convert to datetime. \n",
    "    df[0] += df.pop(1)\n",
    "    df[0] = pd.to_datetime(df[0], format=DATE_FORMAT, errors='coerce')\n",
    "    \n",
    "    # Some rows have year missing from their timestamp cells; however, these rows are always duplicated, due \n",
    "    # to how the data is represented on the website, and can be safely deleted.\n",
    "    df.dropna(subset=0, inplace=True)\n",
    "    \n",
    "    df.columns = COLUMNS\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # You sometimes get a rogue reading from the day following whatever day you run it on, \n",
    "    # so, if limits are set, this gets rid of this faulty reading. \n",
    "    if start_date is not None or end_date is not None: \n",
    "        df = df[\n",
    "            (df.index >= (df.index.min() if start_date is None else start_date)) & \n",
    "            (df.index <= (df.index.max() if end_date is None else end_date))\n",
    "        ]\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "267cf596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(start_date: str, end_date: str, *, postprocess=lambda x: x, preprocess=lambda x: x) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convenience function for (optionally) preprocessing each dataframe from scrape_gen, concatenating them \n",
    "    together, and (optionally) applying a postprocessing function to the concatenatenated dataframe. \n",
    "    \"\"\"\n",
    "    return postprocess(pd.concat(map(preprocess, scrape_gen(start_date=start_date, end_date=end_date))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc2036",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape('08/08/2018', '08/11/2018')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
